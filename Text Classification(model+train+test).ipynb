{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages and pathes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main path for provided data\n",
    "gloveFile = 'D:/SentimentAnalysis/glove.6B.300d.txt/glove.6B.300d.txt'\n",
    "vocab_path = 'D:/SentimentAnalysis/glove.6B.300d.txt/vocab_glove.csv'\n",
    "# processed data path\n",
    "path = 'D:/SentimentAnalysis/stanfordSentimentTreebank/stanfordSentimentTreebank/'\n",
    "train_data_path ='D:/SentimentAnalysis/Train/train.csv'\n",
    "val_data_path ='D:/SentimentAnalysis/Train/val.csv'\n",
    "test_data_path ='D:/SentimentAnalysis/Train/test.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>phrase_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>! '</td>\n",
       "      <td>22935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>! ''</td>\n",
       "      <td>18235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>! Alas</td>\n",
       "      <td>179257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>! Brilliant</td>\n",
       "      <td>22936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239227</th>\n",
       "      <td>zoning ordinances to protect your community fr...</td>\n",
       "      <td>220441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239228</th>\n",
       "      <td>zzzzzzzzz</td>\n",
       "      <td>179256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239229</th>\n",
       "      <td>élan</td>\n",
       "      <td>220442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239230</th>\n",
       "      <td>É</td>\n",
       "      <td>220443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239231</th>\n",
       "      <td>É um passatempo descompromissado</td>\n",
       "      <td>220444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>239232 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Phrase phrase_ids\n",
       "0                                                       !          0\n",
       "1                                                     ! '      22935\n",
       "2                                                    ! ''      18235\n",
       "3                                                  ! Alas     179257\n",
       "4                                             ! Brilliant      22936\n",
       "...                                                   ...        ...\n",
       "239227  zoning ordinances to protect your community fr...     220441\n",
       "239228                                          zzzzzzzzz     179256\n",
       "239229                                               élan     220442\n",
       "239230                                                  É     220443\n",
       "239231                   É um passatempo descompromissado     220444\n",
       "\n",
       "[239232 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process the data with phrase and index separation\n",
    "df_data_sentence = pd.read_table(path + 'dictionary.txt')\n",
    "df_data_sentence_processed = df_data_sentence['p|id'].str.split('|', expand=True)\n",
    "df_data_sentence_processed = df_data_sentence_processed.rename(columns={0: 'Phrase', 1: 'phrase_ids'})\n",
    "df_data_sentence_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase_ids</th>\n",
       "      <th>sentiment_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.44444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.42708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239227</th>\n",
       "      <td>239227</td>\n",
       "      <td>0.36111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239228</th>\n",
       "      <td>239228</td>\n",
       "      <td>0.38889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239229</th>\n",
       "      <td>239229</td>\n",
       "      <td>0.33333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239230</th>\n",
       "      <td>239230</td>\n",
       "      <td>0.88889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239231</th>\n",
       "      <td>239231</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>239232 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       phrase_ids sentiment_values\n",
       "0               0              0.5\n",
       "1               1              0.5\n",
       "2               2          0.44444\n",
       "3               3              0.5\n",
       "4               4          0.42708\n",
       "...           ...              ...\n",
       "239227     239227          0.36111\n",
       "239228     239228          0.38889\n",
       "239229     239229          0.33333\n",
       "239230     239230          0.88889\n",
       "239231     239231              0.5\n",
       "\n",
       "[239232 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read sentiment labels into df\n",
    "df_data_sentiment = pd.read_table(path + 'sentiment_labels.txt')\n",
    "df_data_sentiment_processed = df_data_sentiment['phrase ids|sentiment values'].str.split('|', expand=True)\n",
    "df_data_sentiment_processed = df_data_sentiment_processed.rename(columns={0: 'phrase_ids', 1: 'sentiment_values'})\n",
    "df_data_sentiment_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>phrase_ids</th>\n",
       "      <th>sentiment_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>! '</td>\n",
       "      <td>22935</td>\n",
       "      <td>0.52778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>! ''</td>\n",
       "      <td>18235</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>! Alas</td>\n",
       "      <td>179257</td>\n",
       "      <td>0.44444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>! Brilliant</td>\n",
       "      <td>22936</td>\n",
       "      <td>0.86111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239227</th>\n",
       "      <td>zoning ordinances to protect your community fr...</td>\n",
       "      <td>220441</td>\n",
       "      <td>0.13889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239228</th>\n",
       "      <td>zzzzzzzzz</td>\n",
       "      <td>179256</td>\n",
       "      <td>0.19444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239229</th>\n",
       "      <td>élan</td>\n",
       "      <td>220442</td>\n",
       "      <td>0.51389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239230</th>\n",
       "      <td>É</td>\n",
       "      <td>220443</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239231</th>\n",
       "      <td>É um passatempo descompromissado</td>\n",
       "      <td>220444</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>239232 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Phrase phrase_ids  \\\n",
       "0                                                       !          0   \n",
       "1                                                     ! '      22935   \n",
       "2                                                    ! ''      18235   \n",
       "3                                                  ! Alas     179257   \n",
       "4                                             ! Brilliant      22936   \n",
       "...                                                   ...        ...   \n",
       "239227  zoning ordinances to protect your community fr...     220441   \n",
       "239228                                          zzzzzzzzz     179256   \n",
       "239229                                               élan     220442   \n",
       "239230                                                  É     220443   \n",
       "239231                   É um passatempo descompromissado     220444   \n",
       "\n",
       "       sentiment_values  \n",
       "0                   0.5  \n",
       "1               0.52778  \n",
       "2                   0.5  \n",
       "3               0.44444  \n",
       "4               0.86111  \n",
       "...                 ...  \n",
       "239227          0.13889  \n",
       "239228          0.19444  \n",
       "239229          0.51389  \n",
       "239230              0.5  \n",
       "239231              0.5  \n",
       "\n",
       "[239232 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# processed all data\n",
    "df_processed_all = df_data_sentence_processed.merge(df_data_sentiment_processed, how='inner', on='phrase_ids')\n",
    "df_processed_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate all the data into 3 parts, 80% train, 10% valid, 10% test\n",
    "def data_split(all_data,splitPercent):\n",
    "\n",
    "    m = np.random.rand(len(all_data)) < splitPercent\n",
    "    train = all_data[m]\n",
    "    test_and_dev = all_data[~m]\n",
    "\n",
    "\n",
    "    m_test = np.random.rand(len(test_and_dev)) <0.5\n",
    "    test = test_and_dev[m_test]\n",
    "    dev = test_and_dev[~m_test]\n",
    "\n",
    "    dev.to_csv(path+ 'Train/dev.csv')\n",
    "    test.to_csv(path+  'Train/test.csv')\n",
    "    train.to_csv(path + 'Train/train.csv' )\n",
    "\n",
    "    return train, test, dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = df_processed_all\n",
    "train_data, test_data, dev_data = data_split(all_data, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.reset_index()\n",
    "dev_data = dev_data.reset_index()\n",
    "test_data = test_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>phrase_ids</th>\n",
       "      <th>sentiment_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>! '</td>\n",
       "      <td>22935</td>\n",
       "      <td>0.52778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>! Alas</td>\n",
       "      <td>179257</td>\n",
       "      <td>0.44444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>! Brilliant</td>\n",
       "      <td>22936</td>\n",
       "      <td>0.86111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>! Brilliant !</td>\n",
       "      <td>40532</td>\n",
       "      <td>0.93056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191167</th>\n",
       "      <td>239227</td>\n",
       "      <td>zoning ordinances to protect your community fr...</td>\n",
       "      <td>220441</td>\n",
       "      <td>0.13889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191168</th>\n",
       "      <td>239228</td>\n",
       "      <td>zzzzzzzzz</td>\n",
       "      <td>179256</td>\n",
       "      <td>0.19444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191169</th>\n",
       "      <td>239229</td>\n",
       "      <td>élan</td>\n",
       "      <td>220442</td>\n",
       "      <td>0.51389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191170</th>\n",
       "      <td>239230</td>\n",
       "      <td>É</td>\n",
       "      <td>220443</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191171</th>\n",
       "      <td>239231</td>\n",
       "      <td>É um passatempo descompromissado</td>\n",
       "      <td>220444</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191172 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index                                             Phrase phrase_ids  \\\n",
       "0            0                                                  !          0   \n",
       "1            1                                                ! '      22935   \n",
       "2            3                                             ! Alas     179257   \n",
       "3            4                                        ! Brilliant      22936   \n",
       "4            5                                      ! Brilliant !      40532   \n",
       "...        ...                                                ...        ...   \n",
       "191167  239227  zoning ordinances to protect your community fr...     220441   \n",
       "191168  239228                                          zzzzzzzzz     179256   \n",
       "191169  239229                                               élan     220442   \n",
       "191170  239230                                                  É     220443   \n",
       "191171  239231                   É um passatempo descompromissado     220444   \n",
       "\n",
       "       sentiment_values  \n",
       "0                   0.5  \n",
       "1               0.52778  \n",
       "2               0.44444  \n",
       "3               0.86111  \n",
       "4               0.93056  \n",
       "...                 ...  \n",
       "191167          0.13889  \n",
       "191168          0.19444  \n",
       "191169          0.51389  \n",
       "191170              0.5  \n",
       "191171              0.5  \n",
       "\n",
       "[191172 rows x 4 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter and load embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_path = path + 'SOStr.txt'\n",
    "filtered_glove_path = path +  'filtered_glove.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the glove pretrained embeddings\n",
    "def filter_glove(full_glove_path):\n",
    "    vocab = set()\n",
    "    with open(sentence_path,'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "        # Drop the trailing newline and strip backslashes. Split into words.\n",
    "            vocab.update(line.strip().replace('\\\\', '').split('|'))\n",
    "    read = 0\n",
    "    wrote = 0\n",
    "    with open(full_glove_path, 'r', encoding='utf-8') as f:\n",
    "        with open(filtered_glove_path, 'w', encoding='utf-8') as out:\n",
    "            for line in f:\n",
    "                read += 1\n",
    "                line = line.strip()\n",
    "                if not line: continue\n",
    "                if line.split(u' ', 1)[0] in vocab:\n",
    "                    out.write(line + '\\n')\n",
    "                    wrote += 1\n",
    "    print('read %s lines, wrote %s' % (read, wrote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 400001 lines, wrote 14941\n"
     ]
    }
   ],
   "source": [
    "filter_glove(gloveFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings from pretrained glove embeddings\n",
    "def load_embeddings(embedding_path):\n",
    "  #Loads embedings, returns weight matrix and dict from words to indices.\n",
    "    print('loading word embeddings from %s' % embedding_path)\n",
    "    weight_vectors = []\n",
    "    word_idx = {}\n",
    "    with open(embedding_path, 'r',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word, vec = line.split(u' ', 1)\n",
    "            word_idx[word] = len(weight_vectors)\n",
    "            weight_vectors.append(np.array(vec.split(), dtype=np.float32))\n",
    "    # '(' and ')' are replaced by '-LRB-' and '-RRB-' respectively in the parse-trees.\n",
    "    word_idx[u'-LRB-'] = word_idx.pop(u'(')\n",
    "    word_idx[u'-RRB-'] = word_idx.pop(u')')\n",
    "    # Random embedding vector for unknown words.\n",
    "    weight_vectors.append(np.random.uniform(-0.05, 0.05, weight_vectors[0].shape).astype(np.float32))\n",
    "    return np.stack(weight_vectors), word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings from D:/SentimentAnalysis/stanfordSentimentTreebank/stanfordSentimentTreebank/filtered_glove.txt\n"
     ]
    }
   ],
   "source": [
    "weight_matrix, word_idx = load_embeddings(filtered_glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0,\n",
       " ',': 1,\n",
       " '.': 2,\n",
       " 'of': 3,\n",
       " 'to': 4,\n",
       " 'and': 5,\n",
       " 'in': 6,\n",
       " 'a': 7,\n",
       " \"'s\": 8,\n",
       " 'for': 9,\n",
       " '-': 10,\n",
       " 'that': 11,\n",
       " 'on': 12,\n",
       " 'is': 13,\n",
       " 'was': 14,\n",
       " 'said': 15,\n",
       " 'with': 16,\n",
       " 'he': 17,\n",
       " 'as': 18,\n",
       " 'it': 19,\n",
       " 'by': 20,\n",
       " 'at': 21,\n",
       " 'from': 24,\n",
       " 'his': 25,\n",
       " \"''\": 26,\n",
       " '``': 27,\n",
       " 'an': 28,\n",
       " 'be': 29,\n",
       " 'has': 30,\n",
       " 'are': 31,\n",
       " 'have': 32,\n",
       " 'but': 33,\n",
       " 'were': 34,\n",
       " 'not': 35,\n",
       " 'this': 36,\n",
       " 'who': 37,\n",
       " 'they': 38,\n",
       " 'had': 39,\n",
       " 'which': 40,\n",
       " 'will': 41,\n",
       " 'their': 42,\n",
       " ':': 43,\n",
       " 'or': 44,\n",
       " 'its': 45,\n",
       " 'one': 46,\n",
       " 'after': 47,\n",
       " 'new': 48,\n",
       " 'been': 49,\n",
       " 'also': 50,\n",
       " 'we': 51,\n",
       " 'would': 52,\n",
       " 'two': 53,\n",
       " 'more': 54,\n",
       " \"'\": 55,\n",
       " 'first': 56,\n",
       " 'about': 57,\n",
       " 'up': 58,\n",
       " 'when': 59,\n",
       " 'year': 60,\n",
       " 'there': 61,\n",
       " 'all': 62,\n",
       " '--': 63,\n",
       " 'out': 64,\n",
       " 'she': 65,\n",
       " 'other': 66,\n",
       " 'people': 67,\n",
       " \"n't\": 68,\n",
       " 'her': 69,\n",
       " 'than': 70,\n",
       " 'over': 71,\n",
       " 'into': 72,\n",
       " 'last': 73,\n",
       " 'some': 74,\n",
       " 'government': 75,\n",
       " 'time': 76,\n",
       " '$': 77,\n",
       " 'you': 78,\n",
       " 'years': 79,\n",
       " 'if': 80,\n",
       " 'no': 81,\n",
       " 'world': 82,\n",
       " 'can': 83,\n",
       " 'three': 84,\n",
       " 'do': 85,\n",
       " ';': 86,\n",
       " 'president': 87,\n",
       " 'only': 88,\n",
       " 'state': 89,\n",
       " 'million': 90,\n",
       " 'could': 91,\n",
       " 'us': 92,\n",
       " 'most': 93,\n",
       " 'against': 94,\n",
       " 'so': 95,\n",
       " 'them': 96,\n",
       " 'what': 97,\n",
       " 'him': 98,\n",
       " 'during': 99,\n",
       " 'before': 100,\n",
       " 'may': 101,\n",
       " 'since': 102,\n",
       " 'many': 103,\n",
       " 'while': 104,\n",
       " 'where': 105,\n",
       " 'states': 106,\n",
       " 'because': 107,\n",
       " 'now': 108,\n",
       " 'city': 109,\n",
       " 'made': 110,\n",
       " 'like': 111,\n",
       " 'between': 112,\n",
       " 'did': 113,\n",
       " 'just': 114,\n",
       " 'national': 115,\n",
       " 'day': 116,\n",
       " 'country': 117,\n",
       " 'under': 118,\n",
       " 'such': 119,\n",
       " 'second': 120,\n",
       " 'then': 121,\n",
       " 'company': 122,\n",
       " 'group': 123,\n",
       " 'any': 124,\n",
       " 'through': 125,\n",
       " 'china': 126,\n",
       " 'four': 127,\n",
       " 'being': 128,\n",
       " 'down': 129,\n",
       " 'war': 130,\n",
       " 'back': 131,\n",
       " 'off': 132,\n",
       " 'police': 133,\n",
       " 'well': 134,\n",
       " 'including': 135,\n",
       " 'team': 136,\n",
       " 'international': 137,\n",
       " 'week': 138,\n",
       " 'still': 139,\n",
       " 'both': 140,\n",
       " 'even': 141,\n",
       " 'high': 142,\n",
       " 'part': 143,\n",
       " 'told': 144,\n",
       " 'those': 145,\n",
       " 'end': 146,\n",
       " 'former': 147,\n",
       " 'these': 148,\n",
       " 'make': 149,\n",
       " 'work': 150,\n",
       " 'our': 151,\n",
       " 'home': 152,\n",
       " 'school': 153,\n",
       " 'party': 154,\n",
       " 'house': 155,\n",
       " 'old': 156,\n",
       " 'later': 157,\n",
       " 'get': 158,\n",
       " 'another': 159,\n",
       " 'news': 160,\n",
       " 'long': 161,\n",
       " 'five': 162,\n",
       " 'called': 163,\n",
       " '1': 164,\n",
       " 'military': 165,\n",
       " 'way': 166,\n",
       " 'used': 167,\n",
       " 'much': 168,\n",
       " 'next': 169,\n",
       " 'game': 170,\n",
       " 'here': 171,\n",
       " '?': 172,\n",
       " 'should': 173,\n",
       " 'take': 174,\n",
       " 'very': 175,\n",
       " 'my': 176,\n",
       " 'security': 177,\n",
       " 'season': 178,\n",
       " 'how': 179,\n",
       " 'public': 180,\n",
       " 'early': 181,\n",
       " 'according': 182,\n",
       " 'several': 183,\n",
       " 'court': 184,\n",
       " 'say': 185,\n",
       " 'around': 186,\n",
       " 'foreign': 187,\n",
       " '10': 188,\n",
       " 'until': 189,\n",
       " 'set': 190,\n",
       " 'political': 191,\n",
       " 'says': 192,\n",
       " 'market': 193,\n",
       " 'however': 194,\n",
       " 'family': 195,\n",
       " 'life': 196,\n",
       " 'same': 197,\n",
       " 'general': 198,\n",
       " 'left': 199,\n",
       " 'good': 200,\n",
       " 'top': 201,\n",
       " 'university': 202,\n",
       " 'going': 203,\n",
       " 'number': 204,\n",
       " 'major': 205,\n",
       " 'known': 206,\n",
       " 'points': 207,\n",
       " 'won': 208,\n",
       " 'six': 209,\n",
       " 'month': 210,\n",
       " 'dollars': 211,\n",
       " 'bank': 212,\n",
       " '2': 213,\n",
       " 'use': 214,\n",
       " 'members': 215,\n",
       " 'each': 216,\n",
       " 'area': 217,\n",
       " 'found': 218,\n",
       " 'official': 219,\n",
       " 'place': 220,\n",
       " 'go': 221,\n",
       " 'based': 222,\n",
       " 'among': 223,\n",
       " 'third': 224,\n",
       " 'times': 225,\n",
       " 'took': 226,\n",
       " 'right': 227,\n",
       " 'days': 228,\n",
       " 'local': 229,\n",
       " 'economic': 230,\n",
       " 'see': 231,\n",
       " 'best': 232,\n",
       " 'report': 233,\n",
       " 'killed': 234,\n",
       " 'held': 235,\n",
       " 'business': 236,\n",
       " 'does': 237,\n",
       " 'own': 238,\n",
       " '%': 239,\n",
       " 'came': 240,\n",
       " 'law': 241,\n",
       " 'months': 242,\n",
       " 'women': 243,\n",
       " \"'re\": 244,\n",
       " 'power': 245,\n",
       " 'think': 246,\n",
       " 'service': 247,\n",
       " 'children': 248,\n",
       " 'show': 249,\n",
       " '/': 250,\n",
       " 'help': 251,\n",
       " 'chief': 252,\n",
       " 'system': 253,\n",
       " 'support': 254,\n",
       " 'series': 255,\n",
       " 'play': 256,\n",
       " 'office': 257,\n",
       " 'following': 258,\n",
       " 'me': 259,\n",
       " 'meeting': 260,\n",
       " 'expected': 261,\n",
       " 'late': 262,\n",
       " 'games': 263,\n",
       " 'league': 264,\n",
       " 'reported': 265,\n",
       " 'final': 266,\n",
       " 'added': 267,\n",
       " 'without': 268,\n",
       " 'white': 269,\n",
       " 'history': 270,\n",
       " 'man': 271,\n",
       " 'men': 272,\n",
       " 'became': 273,\n",
       " 'want': 274,\n",
       " 'march': 275,\n",
       " 'case': 276,\n",
       " 'few': 277,\n",
       " 'run': 278,\n",
       " 'money': 279,\n",
       " 'began': 280,\n",
       " 'open': 281,\n",
       " 'name': 282,\n",
       " 'trade': 283,\n",
       " 'center': 284,\n",
       " '3': 285,\n",
       " 'oil': 286,\n",
       " 'too': 287,\n",
       " 'film': 288,\n",
       " 'win': 289,\n",
       " 'led': 290,\n",
       " 'east': 291,\n",
       " 'central': 292,\n",
       " '20': 293,\n",
       " 'air': 294,\n",
       " 'come': 295,\n",
       " 'town': 296,\n",
       " 'leader': 297,\n",
       " 'army': 298,\n",
       " 'line': 299,\n",
       " 'never': 300,\n",
       " 'little': 301,\n",
       " 'played': 302,\n",
       " 'prime': 303,\n",
       " 'death': 304,\n",
       " 'least': 305,\n",
       " 'put': 306,\n",
       " 'forces': 307,\n",
       " 'past': 308,\n",
       " 'de': 309,\n",
       " 'half': 310,\n",
       " 'saying': 311,\n",
       " 'know': 312,\n",
       " 'peace': 313,\n",
       " 'earlier': 314,\n",
       " 'force': 315,\n",
       " 'great': 316,\n",
       " 'near': 317,\n",
       " 'released': 318,\n",
       " 'small': 319,\n",
       " 'department': 320,\n",
       " 'every': 321,\n",
       " 'health': 322,\n",
       " 'head': 323,\n",
       " 'ago': 324,\n",
       " 'night': 325,\n",
       " 'big': 326,\n",
       " 'cup': 327,\n",
       " 'election': 328,\n",
       " 'region': 329,\n",
       " 'director': 330,\n",
       " 'talks': 331,\n",
       " 'program': 332,\n",
       " 'far': 333,\n",
       " 'today': 334,\n",
       " 'statement': 335,\n",
       " 'although': 336,\n",
       " 'again': 337,\n",
       " 'born': 338,\n",
       " 'development': 339,\n",
       " 'close': 340,\n",
       " 'record': 341,\n",
       " 'along': 342,\n",
       " 'went': 343,\n",
       " 'point': 344,\n",
       " 'must': 345,\n",
       " 'your': 346,\n",
       " 'member': 347,\n",
       " 'plan': 348,\n",
       " 'financial': 349,\n",
       " 'recent': 350,\n",
       " 'campaign': 351,\n",
       " 'become': 352,\n",
       " 'whether': 353,\n",
       " 'lost': 354,\n",
       " 'music': 355,\n",
       " '15': 356,\n",
       " 'got': 357,\n",
       " '30': 358,\n",
       " 'need': 359,\n",
       " '4': 360,\n",
       " 'lead': 361,\n",
       " 'already': 362,\n",
       " 'though': 363,\n",
       " 'might': 364,\n",
       " 'free': 365,\n",
       " 'hit': 366,\n",
       " 'rights': 367,\n",
       " '11': 368,\n",
       " 'information': 369,\n",
       " 'away': 370,\n",
       " '12': 371,\n",
       " '5': 372,\n",
       " 'others': 373,\n",
       " 'control': 374,\n",
       " 'within': 375,\n",
       " 'large': 376,\n",
       " 'economy': 377,\n",
       " 'press': 378,\n",
       " 'agency': 379,\n",
       " 'water': 380,\n",
       " 'died': 381,\n",
       " 'career': 382,\n",
       " 'making': 383,\n",
       " '...': 384,\n",
       " 'deal': 385,\n",
       " 'attack': 386,\n",
       " 'side': 387,\n",
       " 'seven': 388,\n",
       " 'better': 389,\n",
       " 'less': 390,\n",
       " 'once': 391,\n",
       " 'main': 392,\n",
       " 'due': 393,\n",
       " 'building': 394,\n",
       " 'club': 395,\n",
       " 'decision': 396,\n",
       " 'stock': 397,\n",
       " 'given': 398,\n",
       " 'give': 399,\n",
       " 'often': 400,\n",
       " 'television': 401,\n",
       " 'industry': 402,\n",
       " 'order': 403,\n",
       " 'young': 404,\n",
       " \"'ve\": 405,\n",
       " 'age': 406,\n",
       " 'start': 407,\n",
       " 'administration': 408,\n",
       " 'round': 409,\n",
       " 'nations': 410,\n",
       " \"'m\": 411,\n",
       " 'human': 412,\n",
       " 'defense': 413,\n",
       " 'asked': 414,\n",
       " 'total': 415,\n",
       " 'players': 416,\n",
       " 'bill': 417,\n",
       " 'important': 418,\n",
       " 'southern': 419,\n",
       " 'move': 420,\n",
       " 'fire': 421,\n",
       " 'population': 422,\n",
       " 'include': 423,\n",
       " 'further': 424,\n",
       " 'nuclear': 425,\n",
       " 'street': 426,\n",
       " 'taken': 427,\n",
       " 'media': 428,\n",
       " 'different': 429,\n",
       " 'issue': 430,\n",
       " 'received': 431,\n",
       " 'secretary': 432,\n",
       " 'return': 433,\n",
       " 'college': 434,\n",
       " 'working': 435,\n",
       " 'community': 436,\n",
       " 'eight': 437,\n",
       " 'groups': 438,\n",
       " 'despite': 439,\n",
       " 'level': 440,\n",
       " 'whose': 441,\n",
       " 'attacks': 442,\n",
       " 'change': 443,\n",
       " 'church': 444,\n",
       " 'nation': 445,\n",
       " 'station': 446,\n",
       " 'weeks': 447,\n",
       " 'having': 448,\n",
       " '18': 449,\n",
       " 'research': 450,\n",
       " 'black': 451,\n",
       " 'services': 452,\n",
       " 'story': 453,\n",
       " '6': 454,\n",
       " 'sales': 455,\n",
       " 'policy': 456,\n",
       " 'visit': 457,\n",
       " 'northern': 458,\n",
       " 'lot': 459,\n",
       " 'across': 460,\n",
       " 'per': 461,\n",
       " 'current': 462,\n",
       " 'board': 463,\n",
       " 'football': 464,\n",
       " 'workers': 465,\n",
       " 'vote': 466,\n",
       " 'book': 467,\n",
       " 'fell': 468,\n",
       " 'seen': 469,\n",
       " 'role': 470,\n",
       " 'students': 471,\n",
       " 'shares': 472,\n",
       " 'process': 473,\n",
       " 'agreement': 474,\n",
       " 'quarter': 475,\n",
       " 'full': 476,\n",
       " 'match': 477,\n",
       " 'started': 478,\n",
       " 'growth': 479,\n",
       " 'yet': 480,\n",
       " 'moved': 481,\n",
       " 'possible': 482,\n",
       " 'western': 483,\n",
       " 'special': 484,\n",
       " '100': 485,\n",
       " 'plans': 486,\n",
       " 'interest': 487,\n",
       " 'behind': 488,\n",
       " 'strong': 489,\n",
       " 'named': 490,\n",
       " 'food': 491,\n",
       " 'period': 492,\n",
       " 'real': 493,\n",
       " 'car': 494,\n",
       " 'term': 495,\n",
       " 'rate': 496,\n",
       " 'race': 497,\n",
       " 'nearly': 498,\n",
       " 'enough': 499,\n",
       " 'keep': 500,\n",
       " '25': 501,\n",
       " 'call': 502,\n",
       " 'future': 503,\n",
       " 'taking': 504,\n",
       " 'island': 505,\n",
       " 'road': 506,\n",
       " 'outside': 507,\n",
       " 'really': 508,\n",
       " 'century': 509,\n",
       " 'democratic': 510,\n",
       " 'almost': 511,\n",
       " 'single': 512,\n",
       " 'share': 513,\n",
       " 'leading': 514,\n",
       " 'trying': 515,\n",
       " 'find': 516,\n",
       " 'album': 517,\n",
       " 'senior': 518,\n",
       " 'minutes': 519,\n",
       " 'together': 520,\n",
       " 'results': 521,\n",
       " 'hard': 522,\n",
       " 'hours': 523,\n",
       " 'land': 524,\n",
       " 'action': 525,\n",
       " 'higher': 526,\n",
       " 'field': 527,\n",
       " 'cut': 528,\n",
       " 'issues': 529,\n",
       " 'executive': 530,\n",
       " 'production': 531,\n",
       " 'areas': 532,\n",
       " 'river': 533,\n",
       " 'face': 534,\n",
       " 'using': 535,\n",
       " 'province': 536,\n",
       " 'park': 537,\n",
       " 'price': 538,\n",
       " 'father': 539,\n",
       " 'son': 540,\n",
       " 'education': 541,\n",
       " '7': 542,\n",
       " 'village': 543,\n",
       " 'energy': 544,\n",
       " 'shot': 545,\n",
       " 'short': 546,\n",
       " 'key': 547,\n",
       " 'red': 548,\n",
       " 'average': 549,\n",
       " 'pay': 550,\n",
       " 'exchange': 551,\n",
       " 'something': 552,\n",
       " 'gave': 553,\n",
       " 'likely': 554,\n",
       " 'player': 555,\n",
       " '8': 556,\n",
       " 'low': 557,\n",
       " 'things': 558,\n",
       " 'post': 559,\n",
       " 'social': 560,\n",
       " 'continue': 561,\n",
       " 'ever': 562,\n",
       " 'look': 563,\n",
       " 'job': 564,\n",
       " '2000': 565,\n",
       " 'soldiers': 566,\n",
       " 'able': 567,\n",
       " 'front': 568,\n",
       " 'himself': 569,\n",
       " 'problems': 570,\n",
       " 'private': 571,\n",
       " 'lower': 572,\n",
       " 'list': 573,\n",
       " 'built': 574,\n",
       " '13': 575,\n",
       " 'efforts': 576,\n",
       " 'dollar': 577,\n",
       " 'miles': 578,\n",
       " 'included': 579,\n",
       " 'radio': 580,\n",
       " 'live': 581,\n",
       " 'form': 582,\n",
       " 'increase': 583,\n",
       " 'reports': 584,\n",
       " 'sent': 585,\n",
       " 'fourth': 586,\n",
       " 'always': 587,\n",
       " '50': 588,\n",
       " 'tax': 589,\n",
       " 'playing': 590,\n",
       " 'title': 591,\n",
       " 'middle': 592,\n",
       " 'meet': 593,\n",
       " 'wife': 594,\n",
       " 'position': 595,\n",
       " 'clear': 596,\n",
       " 'ahead': 597,\n",
       " 'result': 598,\n",
       " 'release': 599,\n",
       " 'violence': 600,\n",
       " 'goal': 601,\n",
       " 'project': 602,\n",
       " 'closed': 603,\n",
       " 'border': 604,\n",
       " 'body': 605,\n",
       " 'soon': 606,\n",
       " 'crisis': 607,\n",
       " 'division': 608,\n",
       " 'served': 609,\n",
       " 'tour': 610,\n",
       " 'hospital': 611,\n",
       " 'test': 612,\n",
       " 'technology': 613,\n",
       " 'believe': 614,\n",
       " 'published': 615,\n",
       " 'agreed': 616,\n",
       " 'why': 617,\n",
       " 'nine': 618,\n",
       " 'summer': 619,\n",
       " 'wanted': 620,\n",
       " 'act': 621,\n",
       " 'recently': 622,\n",
       " 'course': 623,\n",
       " 'problem': 624,\n",
       " 'medical': 625,\n",
       " 'done': 626,\n",
       " 'reached': 627,\n",
       " 'star': 628,\n",
       " 'continued': 629,\n",
       " 'living': 630,\n",
       " 'care': 631,\n",
       " 'art': 632,\n",
       " 'provide': 633,\n",
       " 'worked': 634,\n",
       " 'gold': 635,\n",
       " 'morning': 636,\n",
       " 'dead': 637,\n",
       " 'opened': 638,\n",
       " \"'ll\": 639,\n",
       " 'event': 640,\n",
       " 'previous': 641,\n",
       " 'cost': 642,\n",
       " 'instead': 643,\n",
       " 'band': 644,\n",
       " 'teams': 645,\n",
       " 'daily': 646,\n",
       " '2001': 647,\n",
       " 'available': 648,\n",
       " 'drug': 649,\n",
       " 'coming': 650,\n",
       " 'investment': 651,\n",
       " 'civil': 652,\n",
       " 'woman': 653,\n",
       " 'training': 654,\n",
       " 'appeared': 655,\n",
       " '9': 656,\n",
       " 'involved': 657,\n",
       " 'similar': 658,\n",
       " 'situation': 659,\n",
       " 'running': 660,\n",
       " 'fighting': 661,\n",
       " 'mark': 662,\n",
       " '40': 663,\n",
       " 'trial': 664,\n",
       " 'hold': 665,\n",
       " 'thought': 666,\n",
       " '!': 667,\n",
       " 'study': 668,\n",
       " 'fall': 669,\n",
       " 'mother': 670,\n",
       " 'met': 671,\n",
       " '2002': 672,\n",
       " 'song': 673,\n",
       " 'popular': 674,\n",
       " 'base': 675,\n",
       " 'tv': 676,\n",
       " 'ground': 677,\n",
       " 'markets': 678,\n",
       " 'saw': 679,\n",
       " 'hand': 680,\n",
       " 'hope': 681,\n",
       " 'pressure': 682,\n",
       " 'legal': 683,\n",
       " 'budget': 684,\n",
       " 'returned': 685,\n",
       " 'considered': 686,\n",
       " 'love': 687,\n",
       " 'wrote': 688,\n",
       " 'stop': 689,\n",
       " 'fight': 690,\n",
       " 'currently': 691,\n",
       " 'charges': 692,\n",
       " 'try': 693,\n",
       " 'aid': 694,\n",
       " 'ended': 695,\n",
       " 'brought': 696,\n",
       " 'cases': 697,\n",
       " 'decided': 698,\n",
       " 'failed': 699,\n",
       " 'network': 700,\n",
       " 'works': 701,\n",
       " 'gas': 702,\n",
       " 'turned': 703,\n",
       " 'fact': 704,\n",
       " 'vice': 705,\n",
       " 'ca': 706,\n",
       " 'trading': 707,\n",
       " 'especially': 708,\n",
       " 'reporters': 709,\n",
       " 'common': 710,\n",
       " 'looking': 711,\n",
       " 'space': 712,\n",
       " 'rates': 713,\n",
       " 'manager': 714,\n",
       " 'loss': 715,\n",
       " 'justice': 716,\n",
       " 'thousands': 717,\n",
       " 'rather': 718,\n",
       " 'thing': 719,\n",
       " 'opening': 720,\n",
       " 'accused': 721,\n",
       " 'winning': 722,\n",
       " 'scored': 723,\n",
       " 'championship': 724,\n",
       " 'example': 725,\n",
       " 'getting': 726,\n",
       " 'biggest': 727,\n",
       " 'performance': 728,\n",
       " 'sports': 729,\n",
       " '1998': 730,\n",
       " 'let': 731,\n",
       " 'allowed': 732,\n",
       " 'schools': 733,\n",
       " 'means': 734,\n",
       " 'turn': 735,\n",
       " 'leave': 736,\n",
       " 'no.': 737,\n",
       " 'personal': 738,\n",
       " 'showed': 739,\n",
       " 'light': 740,\n",
       " 'arrested': 741,\n",
       " 'person': 742,\n",
       " 'either': 743,\n",
       " 'offer': 744,\n",
       " 'majority': 745,\n",
       " 'battle': 746,\n",
       " '19': 747,\n",
       " 'class': 748,\n",
       " 'evidence': 749,\n",
       " 'makes': 750,\n",
       " 'society': 751,\n",
       " 'products': 752,\n",
       " 'needed': 753,\n",
       " 'stage': 754,\n",
       " 'am': 755,\n",
       " 'doing': 756,\n",
       " 'families': 757,\n",
       " 'construction': 758,\n",
       " 'various': 759,\n",
       " 'sold': 760,\n",
       " 'independent': 761,\n",
       " 'kind': 762,\n",
       " 'airport': 763,\n",
       " 'judge': 764,\n",
       " 'internet': 765,\n",
       " 'movement': 766,\n",
       " 'room': 767,\n",
       " 'followed': 768,\n",
       " 'original': 769,\n",
       " '`': 770,\n",
       " 'comes': 771,\n",
       " 'parties': 772,\n",
       " 'nothing': 773,\n",
       " 'sea': 774,\n",
       " 'bring': 775,\n",
       " 'annual': 776,\n",
       " 'officer': 777,\n",
       " 'present': 778,\n",
       " 'remain': 779,\n",
       " '1999': 780,\n",
       " '22': 781,\n",
       " 'remains': 782,\n",
       " 'allow': 783,\n",
       " 'computer': 784,\n",
       " 'contract': 785,\n",
       " 'coast': 786,\n",
       " 'created': 787,\n",
       " 'demand': 788,\n",
       " 'events': 789,\n",
       " 'beat': 790,\n",
       " 'interview': 791,\n",
       " 'helped': 792,\n",
       " 'child': 793,\n",
       " 'probably': 794,\n",
       " 'spent': 795,\n",
       " 'effort': 796,\n",
       " 'shows': 797,\n",
       " 'calls': 798,\n",
       " 'investigation': 799,\n",
       " 'lives': 800,\n",
       " 'video': 801,\n",
       " 'runs': 802,\n",
       " 'tried': 803,\n",
       " 'bad': 804,\n",
       " 'described': 805,\n",
       " '1994': 806,\n",
       " 'toward': 807,\n",
       " 'written': 808,\n",
       " 'throughout': 809,\n",
       " 'established': 810,\n",
       " 'mission': 811,\n",
       " 'associated': 812,\n",
       " 'buy': 813,\n",
       " 'growing': 814,\n",
       " 'green': 815,\n",
       " 'forward': 816,\n",
       " 'competition': 817,\n",
       " 'poor': 818,\n",
       " 'latest': 819,\n",
       " 'question': 820,\n",
       " '1997': 821,\n",
       " 'prison': 822,\n",
       " 'feel': 823,\n",
       " 'attention': 824,\n",
       " 'themselves': 825,\n",
       " 'injured': 826,\n",
       " 'itself': 827,\n",
       " 'movie': 828,\n",
       " 'range': 829,\n",
       " 'cross': 830,\n",
       " 'track': 831,\n",
       " 'programs': 832,\n",
       " '1995': 833,\n",
       " 'forced': 834,\n",
       " 'includes': 835,\n",
       " 'difficult': 836,\n",
       " 'produced': 837,\n",
       " 'wall': 838,\n",
       " 'chance': 839,\n",
       " 'reach': 840,\n",
       " 'adding': 841,\n",
       " 'species': 842,\n",
       " 'wants': 843,\n",
       " 'finished': 844,\n",
       " 'rise': 845,\n",
       " 'killing': 846,\n",
       " 'joined': 847,\n",
       " 'language': 848,\n",
       " 'rest': 849,\n",
       " 'serious': 850,\n",
       " 'inside': 851,\n",
       " 'immediately': 852,\n",
       " 'brown': 853,\n",
       " 'remained': 854,\n",
       " 'parts': 855,\n",
       " 'success': 856,\n",
       " 'changes': 857,\n",
       " 'la': 858,\n",
       " 'residents': 859,\n",
       " 'meanwhile': 860,\n",
       " 'net': 861,\n",
       " 'sides': 862,\n",
       " 'hall': 863,\n",
       " 'believed': 864,\n",
       " 'records': 865,\n",
       " 'heart': 866,\n",
       " 'champion': 867,\n",
       " 'award': 868,\n",
       " 'planned': 869,\n",
       " 'version': 870,\n",
       " 'grand': 871,\n",
       " 'step': 872,\n",
       " 'labor': 873,\n",
       " 'above': 874,\n",
       " 'sometimes': 875,\n",
       " 'addition': 876,\n",
       " 'raised': 877,\n",
       " 'needs': 878,\n",
       " '26': 879,\n",
       " 'talk': 880,\n",
       " 'longer': 881,\n",
       " 'terms': 882,\n",
       " 'rules': 883,\n",
       " 'joint': 884,\n",
       " 'ball': 885,\n",
       " 'beginning': 886,\n",
       " 'sure': 887,\n",
       " 'stay': 888,\n",
       " 'worth': 889,\n",
       " 'charge': 890,\n",
       " 'begin': 891,\n",
       " 'friends': 892,\n",
       " 'anything': 893,\n",
       " 'caused': 894,\n",
       " 'sources': 895,\n",
       " 'sign': 896,\n",
       " 'armed': 897,\n",
       " 'heavy': 898,\n",
       " 'parents': 899,\n",
       " 'spending': 900,\n",
       " 'related': 901,\n",
       " 'offered': 902,\n",
       " 'particularly': 903,\n",
       " 'aircraft': 904,\n",
       " 'whole': 905,\n",
       " '60': 906,\n",
       " 'authority': 907,\n",
       " 'risk': 908,\n",
       " 'matter': 909,\n",
       " 'science': 910,\n",
       " 'access': 911,\n",
       " 'conditions': 912,\n",
       " 'details': 913,\n",
       " 'compared': 914,\n",
       " 'break': 915,\n",
       " 'turkey': 916,\n",
       " 'daughter': 917,\n",
       " 'natural': 918,\n",
       " 'museum': 919,\n",
       " 'strike': 920,\n",
       " 'style': 921,\n",
       " 'paid': 922,\n",
       " 'rock': 923,\n",
       " 'costs': 924,\n",
       " 'view': 925,\n",
       " 'usually': 926,\n",
       " 'travel': 927,\n",
       " 'takes': 928,\n",
       " 'books': 929,\n",
       " 'arrived': 930,\n",
       " 'hundreds': 931,\n",
       " \"'d\": 932,\n",
       " 'hour': 933,\n",
       " 'response': 934,\n",
       " 'developed': 935,\n",
       " 'jones': 936,\n",
       " 'potential': 937,\n",
       " 'cause': 938,\n",
       " 'value': 939,\n",
       " 'idea': 940,\n",
       " '&': 941,\n",
       " 'professional': 942,\n",
       " 'domestic': 943,\n",
       " 'credit': 944,\n",
       " 'changed': 945,\n",
       " 'married': 946,\n",
       " 'web': 947,\n",
       " 'source': 948,\n",
       " 'everything': 949,\n",
       " 'weekend': 950,\n",
       " 'camp': 951,\n",
       " 'quickly': 952,\n",
       " 'owned': 953,\n",
       " 'eventually': 954,\n",
       " 'measures': 955,\n",
       " 'cities': 956,\n",
       " 'blue': 957,\n",
       " 'starting': 958,\n",
       " 'ready': 959,\n",
       " 'plant': 960,\n",
       " 'build': 961,\n",
       " 'designed': 962,\n",
       " 'certain': 963,\n",
       " 'modern': 964,\n",
       " 'fans': 965,\n",
       " 'commercial': 966,\n",
       " 'bid': 967,\n",
       " 'provided': 968,\n",
       " 'dropped': 969,\n",
       " 'brother': 970,\n",
       " 'drive': 971,\n",
       " 'goals': 972,\n",
       " 'affairs': 973,\n",
       " 'unit': 974,\n",
       " 'questions': 975,\n",
       " 'significant': 976,\n",
       " 'create': 977,\n",
       " 'bomb': 978,\n",
       " 'warned': 979,\n",
       " 'religious': 980,\n",
       " 'minute': 981,\n",
       " 'figures': 982,\n",
       " 'victims': 983,\n",
       " 'condition': 984,\n",
       " 'activities': 985,\n",
       " 'upon': 986,\n",
       " 'leaving': 987,\n",
       " 'experience': 988,\n",
       " '=': 989,\n",
       " 'standard': 990,\n",
       " 'intelligence': 991,\n",
       " 'giving': 992,\n",
       " 'hotel': 993,\n",
       " 'finally': 994,\n",
       " 'magazine': 995,\n",
       " 'whom': 996,\n",
       " 'moving': 997,\n",
       " 'carried': 998,\n",
       " 'feet': 999,\n",
       " 'candidate': 1000,\n",
       " 'fifth': 1001,\n",
       " ...}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess without embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate embedding for non-pretrained word embedding\n",
    "def make_dict(data):\n",
    "    word_idx = dict()\n",
    "    index = 0\n",
    "    for idx, row in data.iterrows():\n",
    "        sentence = (row['Phrase'])\n",
    "        #print (sentence)\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        sentence_words = tokenizer.tokenize(sentence)\n",
    "        #print (sentence_words)\n",
    "        for word in sentence_words:\n",
    "            #print(index)\n",
    "            word_lwr = word.lower()\n",
    "            if word_lwr not in word_idx:\n",
    "                word_idx[word_lwr] = index\n",
    "                index += 1\n",
    "    idx_word = {i: w for i, w in enumerate(word_idx)}\n",
    "    print('the number of words : ', len(word_idx))\n",
    "    \n",
    "    return word_idx, idx_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of words :  18057\n"
     ]
    }
   ],
   "source": [
    "word_idx, idx_word = make_dict(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alas': 0,\n",
       " 'brilliant': 1,\n",
       " 'c': 2,\n",
       " 'mon': 3,\n",
       " 'gollum': 4,\n",
       " 's': 5,\n",
       " 'performance': 6,\n",
       " 'is': 7,\n",
       " 'incredible': 8,\n",
       " 'oh': 9,\n",
       " 'look': 10,\n",
       " 'at': 11,\n",
       " 'that': 12,\n",
       " 'clever': 13,\n",
       " 'angle': 14,\n",
       " 'wow': 15,\n",
       " 'a': 16,\n",
       " 'jump': 17,\n",
       " 'cut': 18,\n",
       " 'romething': 19,\n",
       " 'run': 20,\n",
       " 'the': 21,\n",
       " 'movie': 22,\n",
       " 'camera': 23,\n",
       " 'twirls': 24,\n",
       " 'true': 25,\n",
       " 'hollywood': 26,\n",
       " 'story': 27,\n",
       " 'zoom': 28,\n",
       " '133': 29,\n",
       " '3': 30,\n",
       " '8217': 31,\n",
       " 't': 32,\n",
       " '9': 33,\n",
       " '1': 34,\n",
       " '8': 35,\n",
       " 'million': 36,\n",
       " 'charmer': 37,\n",
       " '100': 38,\n",
       " 'on': 39,\n",
       " 'this': 40,\n",
       " '20': 41,\n",
       " 'ticket': 42,\n",
       " 'to': 43,\n",
       " 'ride': 44,\n",
       " 'russian': 45,\n",
       " 'rocket': 46,\n",
       " '40': 47,\n",
       " 'version': 48,\n",
       " '50': 49,\n",
       " 'us': 50,\n",
       " 'budget': 51,\n",
       " '7': 52,\n",
       " '00': 53,\n",
       " 'and': 54,\n",
       " '93': 55,\n",
       " 'minutes': 56,\n",
       " 'of': 57,\n",
       " 'unrecoverable': 58,\n",
       " 'life': 59,\n",
       " '99': 60,\n",
       " 'bargain': 61,\n",
       " 'basement': 62,\n",
       " 'special': 63,\n",
       " 'white': 64,\n",
       " 'freeze': 65,\n",
       " 'frames': 66,\n",
       " 'reminiscent': 67,\n",
       " 'pseudo': 68,\n",
       " 'hip': 69,\n",
       " 'luxury': 70,\n",
       " 'car': 71,\n",
       " 'commercial': 72,\n",
       " 'it': 73,\n",
       " 'its': 74,\n",
       " 'worst': 75,\n",
       " 'when': 76,\n",
       " 'actually': 77,\n",
       " 'inside': 78,\n",
       " 'ring': 79,\n",
       " 'amazing': 80,\n",
       " 'j': 81,\n",
       " 'sandwich': 82,\n",
       " 'stitch': 83,\n",
       " 'n': 84,\n",
       " 'most': 85,\n",
       " 'edgy': 86,\n",
       " 'piece': 87,\n",
       " 'disney': 88,\n",
       " 'animation': 89,\n",
       " 'hit': 90,\n",
       " 'silver': 91,\n",
       " 'screen': 92,\n",
       " 'then': 93,\n",
       " 'first': 94,\n",
       " 'film': 95,\n",
       " 'use': 96,\n",
       " 'watercolor': 97,\n",
       " 'background': 98,\n",
       " 'since': 99,\n",
       " 'dumbo': 100,\n",
       " 'certainly': 101,\n",
       " 'ranks': 102,\n",
       " 'as': 103,\n",
       " 'original': 104,\n",
       " 'in': 105,\n",
       " 'years': 106,\n",
       " 'reach': 107,\n",
       " 'emotion': 108,\n",
       " 'or': 109,\n",
       " 'timelessness': 110,\n",
       " 'great': 111,\n",
       " 'past': 112,\n",
       " 'even': 113,\n",
       " 'more': 114,\n",
       " 'recent': 115,\n",
       " 'successes': 116,\n",
       " 'such': 117,\n",
       " 'mulan': 118,\n",
       " 'tarzan': 119,\n",
       " 'heart': 120,\n",
       " 'rate': 121,\n",
       " 'raising': 122,\n",
       " 'w': 123,\n",
       " 'british': 124,\n",
       " 'comedy': 125,\n",
       " '1999': 126,\n",
       " 'hopkins': 127,\n",
       " 'does': 128,\n",
       " 'so': 129,\n",
       " 'much': 130,\n",
       " 'phone': 131,\n",
       " 'his': 132,\n",
       " 'fax': 133,\n",
       " 'no': 134,\n",
       " 'too': 135,\n",
       " 'committed': 136,\n",
       " 'he': 137,\n",
       " 'gets': 138,\n",
       " 'secretary': 139,\n",
       " 'cockettes': 140,\n",
       " 'provides': 141,\n",
       " 'window': 142,\n",
       " 'into': 143,\n",
       " 'subculture': 144,\n",
       " 'hell': 145,\n",
       " 'bent': 146,\n",
       " 'expressing': 147,\n",
       " 'itself': 148,\n",
       " 'every': 149,\n",
       " 'way': 150,\n",
       " 'imaginable': 151,\n",
       " 'despite': 152,\n",
       " 'lagging': 153,\n",
       " 'near': 154,\n",
       " 'finish': 155,\n",
       " 'line': 156,\n",
       " 'runs': 157,\n",
       " 'good': 158,\n",
       " 'race': 159,\n",
       " 'one': 160,\n",
       " 'will': 161,\n",
       " 'have': 162,\n",
       " 'you': 163,\n",
       " 'edge': 164,\n",
       " 'your': 165,\n",
       " 'seat': 166,\n",
       " 'for': 167,\n",
       " 'long': 168,\n",
       " 'stretches': 169,\n",
       " 'mafia': 170,\n",
       " 'rap': 171,\n",
       " 'stars': 172,\n",
       " 'hood': 173,\n",
       " 'rats': 174,\n",
       " 'butt': 175,\n",
       " 'their': 176,\n",
       " 'ugly': 177,\n",
       " 'heads': 178,\n",
       " 'regurgitation': 179,\n",
       " 'cinematic': 180,\n",
       " 'violence': 181,\n",
       " 'gives': 182,\n",
       " 'brutal': 183,\n",
       " 'birth': 184,\n",
       " 'an': 185,\n",
       " 'unlikely': 186,\n",
       " 'but': 187,\n",
       " 'likable': 188,\n",
       " 'hero': 189,\n",
       " 'participatory': 190,\n",
       " 'spectator': 191,\n",
       " 'sport': 192,\n",
       " 'both': 193,\n",
       " 'hokey': 194,\n",
       " 'super': 195,\n",
       " 'cool': 196,\n",
       " 'definitely': 197,\n",
       " 'not': 198,\n",
       " 'hurry': 199,\n",
       " 'sit': 200,\n",
       " 'back': 201,\n",
       " 'relax': 202,\n",
       " 'few': 203,\n",
       " 'laughs': 204,\n",
       " 'while': 205,\n",
       " 'little': 206,\n",
       " 'ones': 207,\n",
       " 'get': 208,\n",
       " 'fuzzy': 209,\n",
       " 'treat': 210,\n",
       " 'cast': 211,\n",
       " 'portrays': 212,\n",
       " 'cartoon': 213,\n",
       " 'counterparts': 214,\n",
       " 'well': 215,\n",
       " 'quite': 216,\n",
       " 'frankly': 217,\n",
       " 'scoob': 218,\n",
       " 'shag': 219,\n",
       " 'do': 220,\n",
       " 'eat': 221,\n",
       " 'enough': 222,\n",
       " 'during': 223,\n",
       " 'considered': 224,\n",
       " 'approach': 225,\n",
       " 'subject': 226,\n",
       " 'matter': 227,\n",
       " 'calm': 228,\n",
       " 'thoughtful': 229,\n",
       " 'agitprop': 230,\n",
       " 'thinness': 231,\n",
       " 'characterizations': 232,\n",
       " 'makes': 233,\n",
       " 'failure': 234,\n",
       " 'straight': 235,\n",
       " 'drama': 236,\n",
       " 'fresh': 237,\n",
       " 'faced': 238,\n",
       " 'big': 239,\n",
       " 'hearted': 240,\n",
       " 'frequently': 241,\n",
       " 'funny': 242,\n",
       " 'thrill': 243,\n",
       " 'kiddies': 244,\n",
       " 'with': 245,\n",
       " 'eye': 246,\n",
       " 'candy': 247,\n",
       " 'cheeky': 248,\n",
       " 'wit': 249,\n",
       " 'keep': 250,\n",
       " 'parents': 251,\n",
       " 'away': 252,\n",
       " 'from': 253,\n",
       " 'concession': 254,\n",
       " 'stand': 255,\n",
       " 'anderson': 256,\n",
       " 'diary': 257,\n",
       " 'i': 258,\n",
       " 'feel': 259,\n",
       " 'better': 260,\n",
       " 'already': 261,\n",
       " 'know': 262,\n",
       " 'how': 263,\n",
       " 'suffer': 264,\n",
       " 'if': 265,\n",
       " 'see': 266,\n",
       " 'll': 267,\n",
       " 'once': 268,\n",
       " 'had': 269,\n",
       " 'nightmare': 270,\n",
       " 'like': 271,\n",
       " 'should': 272,\n",
       " 'be': 273,\n",
       " 'enjoying': 274,\n",
       " 'speak': 275,\n",
       " 'fluent': 276,\n",
       " 'flatula': 277,\n",
       " 'advises': 278,\n",
       " 'denlopp': 279,\n",
       " 'after': 280,\n",
       " 'rather': 281,\n",
       " 'er': 282,\n",
       " 'bubbly': 283,\n",
       " 'exchange': 284,\n",
       " 'alien': 285,\n",
       " 'deckhand': 286,\n",
       " 'think': 287,\n",
       " 'jia': 288,\n",
       " 'lucas': 289,\n",
       " 'michael': 290,\n",
       " 'moore': 291,\n",
       " 'perfect': 292,\n",
       " 'starting': 293,\n",
       " 'point': 294,\n",
       " 'national': 295,\n",
       " 'conversation': 296,\n",
       " 'about': 297,\n",
       " 'guns': 298,\n",
       " 'fear': 299,\n",
       " 'society': 300,\n",
       " 'tv': 301,\n",
       " 'translation': 302,\n",
       " 'elm': 303,\n",
       " 'street': 304,\n",
       " 'hills': 305,\n",
       " 'family': 306,\n",
       " 'because': 307,\n",
       " 'actors': 308,\n",
       " 'tonight': 309,\n",
       " 'maid': 310,\n",
       " 'lie': 311,\n",
       " 'realistic': 312,\n",
       " 'terrorists': 313,\n",
       " 'triumph': 314,\n",
       " 'geriatric': 315,\n",
       " 'appeared': 316,\n",
       " '1938': 317,\n",
       " 'begins': 318,\n",
       " 'ends': 319,\n",
       " 'scenes': 320,\n",
       " 'terrifying': 321,\n",
       " 'm': 322,\n",
       " 'still': 323,\n",
       " 'stunned': 324,\n",
       " 've': 325,\n",
       " 'decided': 326,\n",
       " 'leave': 327,\n",
       " 'light': 328,\n",
       " 'night': 329,\n",
       " 'now': 330,\n",
       " 'butler': 331,\n",
       " 'cheats': 332,\n",
       " 'retreats': 333,\n",
       " 'comfortable': 334,\n",
       " 'territory': 335,\n",
       " 'bad': 336,\n",
       " 'department': 337,\n",
       " 'documentary': 338,\n",
       " 'dwells': 339,\n",
       " 'crossing': 340,\n",
       " 'over': 341,\n",
       " 'mumbo': 342,\n",
       " 'jumbo': 343,\n",
       " 'manipulative': 344,\n",
       " 'sentimentality': 345,\n",
       " 'sappy': 346,\n",
       " 'dialogue': 347,\n",
       " 'factor': 348,\n",
       " 'filmmaking': 349,\n",
       " 'style': 350,\n",
       " 'films': 351,\n",
       " 'flick': 352,\n",
       " 'girl': 353,\n",
       " 'hanging': 354,\n",
       " 'has': 355,\n",
       " 'progressed': 356,\n",
       " 'nicely': 357,\n",
       " 'wayne': 358,\n",
       " 'some': 359,\n",
       " 'visual': 360,\n",
       " 'imagination': 361,\n",
       " 'elsewhere': 362,\n",
       " 'never': 363,\n",
       " 'been': 364,\n",
       " 'appropriate': 365,\n",
       " 'which': 366,\n",
       " 'reputation': 367,\n",
       " 'famous': 368,\n",
       " 'author': 369,\n",
       " 'who': 370,\n",
       " 'ever': 371,\n",
       " 'lived': 372,\n",
       " 'comes': 373,\n",
       " 'question': 374,\n",
       " 'name': 375,\n",
       " 'delibrately': 376,\n",
       " 'obtuse': 377,\n",
       " 'unapproachable': 378,\n",
       " 'waste': 379,\n",
       " 'performances': 380,\n",
       " 'bus': 381,\n",
       " 'wreck': 382,\n",
       " 'turns': 383,\n",
       " 'banal': 384,\n",
       " 'message': 385,\n",
       " 'choice': 386,\n",
       " 'material': 387,\n",
       " 'convey': 388,\n",
       " 'than': 389,\n",
       " 'what': 390,\n",
       " 'best': 391,\n",
       " 'description': 392,\n",
       " 'meaning': 393,\n",
       " 'beautifully': 394,\n",
       " 'produced': 395,\n",
       " 'sacrifices': 396,\n",
       " 'promise': 397,\n",
       " 'high': 398,\n",
       " 'powered': 399,\n",
       " 'star': 400,\n",
       " 'pedigree': 401,\n",
       " 'kind': 402,\n",
       " 'engaging': 403,\n",
       " 'historical': 404,\n",
       " 'appears': 405,\n",
       " 'given': 406,\n",
       " 'up': 407,\n",
       " 'favor': 408,\n",
       " 'sentimental': 409,\n",
       " 'war': 410,\n",
       " 'movies': 411,\n",
       " 'vein': 412,\n",
       " 'we': 413,\n",
       " 'were': 414,\n",
       " 'soldiers': 415,\n",
       " 'word': 416,\n",
       " 'fisted': 417,\n",
       " 'direction': 418,\n",
       " 'jez': 419,\n",
       " 'butterworth': 420,\n",
       " 'manages': 421,\n",
       " 'blast': 422,\n",
       " 'smallest': 423,\n",
       " 'sensitivities': 424,\n",
       " 'romance': 425,\n",
       " 'clamorous': 426,\n",
       " 'wondrously': 427,\n",
       " 'creative': 428,\n",
       " 'also': 429,\n",
       " 'rocks': 430,\n",
       " 'far': 431,\n",
       " 'self': 432,\n",
       " 'conscious': 433,\n",
       " 'draw': 434,\n",
       " 'deeply': 435,\n",
       " 'world': 436,\n",
       " 'journalism': 437,\n",
       " 'left': 438,\n",
       " 'me': 439,\n",
       " 'very': 440,\n",
       " 'feeling': 441,\n",
       " 'paperbacks': 442,\n",
       " 'police': 443,\n",
       " 'posturing': 444,\n",
       " 'project': 445,\n",
       " 'release': 446,\n",
       " 'revelation': 447,\n",
       " 'revolution': 448,\n",
       " 'rip': 449,\n",
       " 'off': 450,\n",
       " 'romantic': 451,\n",
       " 'sequel': 452,\n",
       " 'shots': 453,\n",
       " 'brought': 454,\n",
       " 'out': 455,\n",
       " 'hibernation': 456,\n",
       " 'show': 457,\n",
       " 'shows': 458,\n",
       " 'level': 459,\n",
       " 'young': 460,\n",
       " 'black': 461,\n",
       " 'manhood': 462,\n",
       " 'touching': 463,\n",
       " 'smart': 464,\n",
       " 'complicated': 465,\n",
       " 'sinks': 466,\n",
       " 'song': 467,\n",
       " 'swims': 468,\n",
       " 'sleeper': 469,\n",
       " 'summer': 470,\n",
       " 'award': 471,\n",
       " 'play': 472,\n",
       " 'partly': 473,\n",
       " 'closed': 474,\n",
       " 'down': 475,\n",
       " 'cinderella': 476,\n",
       " 'ii': 477,\n",
       " 'proves': 478,\n",
       " 'wish': 479,\n",
       " 'studio': 480,\n",
       " 'wallet': 481,\n",
       " 'jerry': 482,\n",
       " 'bruckheimer': 483,\n",
       " 'production': 484,\n",
       " 'else': 485,\n",
       " 'offer': 486,\n",
       " 'schlock': 487,\n",
       " 'filled': 488,\n",
       " 'fairy': 489,\n",
       " 'tale': 490,\n",
       " 'hits': 491,\n",
       " 'new': 492,\n",
       " 'depths': 493,\n",
       " 'unoriginality': 494,\n",
       " 'predictability': 495,\n",
       " 'tries': 496,\n",
       " 'force': 497,\n",
       " 'quirkiness': 498,\n",
       " 'upon': 499,\n",
       " 'audience': 500,\n",
       " 'trilogy': 501,\n",
       " 'undertone': 502,\n",
       " 'variety': 503,\n",
       " 'viewers': 504,\n",
       " 'nod': 505,\n",
       " 'agreement': 506,\n",
       " 'appeal': 507,\n",
       " 'would': 508,\n",
       " 'without': 509,\n",
       " 'vulgarity': 510,\n",
       " 'intelligent': 511,\n",
       " 'affirming': 512,\n",
       " 'script': 513,\n",
       " 'something': 514,\n",
       " 'lost': 515,\n",
       " 'time': 516,\n",
       " 'importance': 517,\n",
       " 'being': 518,\n",
       " 'earnest': 519,\n",
       " 'seems': 520,\n",
       " 'missing': 521,\n",
       " 'deal': 522,\n",
       " 'acerbic': 523,\n",
       " 'repartee': 524,\n",
       " '13': 525,\n",
       " 'conversations': 526,\n",
       " 'holds': 527,\n",
       " 'goodwill': 528,\n",
       " 'close': 529,\n",
       " 'relatively': 530,\n",
       " 'slow': 531,\n",
       " 'come': 532,\n",
       " 'thing': 533,\n",
       " 'examines': 534,\n",
       " 'many': 535,\n",
       " 'different': 536,\n",
       " 'ideas': 537,\n",
       " 'happiness': 538,\n",
       " 'guilt': 539,\n",
       " 'intriguing': 540,\n",
       " 'bit': 541,\n",
       " 'storytelling': 542,\n",
       " '2002': 543,\n",
       " 'walk': 544,\n",
       " 'remember': 545,\n",
       " 'succeeds': 546,\n",
       " 'through': 547,\n",
       " 'sincerity': 548,\n",
       " 'abandon': 549,\n",
       " 'wanting': 550,\n",
       " 'theater': 551,\n",
       " 'american': 552,\n",
       " 'beauty': 553,\n",
       " 'entire': 554,\n",
       " 'researchers': 555,\n",
       " 'quietly': 556,\n",
       " 'reading': 557,\n",
       " 'dusty': 558,\n",
       " 'old': 559,\n",
       " 'letters': 560,\n",
       " 'analyze': 561,\n",
       " 'those': 562,\n",
       " 'crass': 563,\n",
       " 'contrived': 564,\n",
       " 'sequels': 565,\n",
       " 'only': 566,\n",
       " 'fails': 567,\n",
       " 'own': 568,\n",
       " 'second': 569,\n",
       " 'guess': 570,\n",
       " 'affection': 571,\n",
       " 'antwone': 572,\n",
       " 'fisher': 573,\n",
       " 'by': 574,\n",
       " 'numbers': 575,\n",
       " 'effort': 576,\n",
       " 'washington': 577,\n",
       " 'wo': 578,\n",
       " 'rock': 579,\n",
       " 'any': 580,\n",
       " 'boats': 581,\n",
       " 'solid': 582,\n",
       " 'meat': 583,\n",
       " 'potatoes': 584,\n",
       " 'austin': 585,\n",
       " 'powers': 586,\n",
       " 'goldmember': 587,\n",
       " 'right': 588,\n",
       " 'stuff': 589,\n",
       " 'silly': 590,\n",
       " 'entertainment': 591,\n",
       " 'sustain': 592,\n",
       " 'interest': 593,\n",
       " 'end': 594,\n",
       " 'auto': 595,\n",
       " 'focus': 596,\n",
       " 'works': 597,\n",
       " 'unusual': 598,\n",
       " 'biopic': 599,\n",
       " 'document': 600,\n",
       " 'male': 601,\n",
       " 'swingers': 602,\n",
       " 'playboy': 603,\n",
       " 'era': 604,\n",
       " 'operative': 605,\n",
       " 'company': 606,\n",
       " 'mean': 607,\n",
       " 'ballistic': 608,\n",
       " 'ecks': 609,\n",
       " 'vs': 610,\n",
       " 'sever': 611,\n",
       " 'safe': 612,\n",
       " 'children': 613,\n",
       " 'bedknobs': 614,\n",
       " 'broomsticks': 615,\n",
       " 'bellini': 616,\n",
       " 'besotted': 617,\n",
       " 'misbegotten': 618,\n",
       " 'madonna': 619,\n",
       " 'sly': 620,\n",
       " 'amusing': 621,\n",
       " 'laugh': 622,\n",
       " 'gem': 623,\n",
       " 'ultimate': 624,\n",
       " 'real': 625,\n",
       " 'kaputschnik': 626,\n",
       " 'birthday': 627,\n",
       " 'actor': 628,\n",
       " 'foremost': 629,\n",
       " 'bowling': 630,\n",
       " 'brown': 631,\n",
       " 'sugar': 632,\n",
       " 'admirably': 633,\n",
       " 'aspires': 634,\n",
       " 'another': 635,\n",
       " 'man': 636,\n",
       " 'clone': 637,\n",
       " 'weaving': 638,\n",
       " 'theme': 639,\n",
       " 'throughout': 640,\n",
       " 'catch': 641,\n",
       " 'feels': 642,\n",
       " 'capable': 643,\n",
       " 'charming': 644,\n",
       " 'masses': 645,\n",
       " 'power': 646,\n",
       " 'pop': 647,\n",
       " 'induced': 648,\n",
       " 'score': 649,\n",
       " 'moments': 650,\n",
       " 'become': 651,\n",
       " 'spielberg': 652,\n",
       " 'trademark': 653,\n",
       " 'chasing': 654,\n",
       " 'clockstoppers': 655,\n",
       " 'conan': 656,\n",
       " 'cremaster': 657,\n",
       " 'warning': 658,\n",
       " 'serious': 659,\n",
       " 'buffs': 660,\n",
       " 'damned': 661,\n",
       " 'derrida': 662,\n",
       " 'undeniably': 663,\n",
       " 'fascinating': 664,\n",
       " 'playful': 665,\n",
       " 'fellow': 666,\n",
       " 'elling': 667,\n",
       " 'empire': 668,\n",
       " 'extreme': 669,\n",
       " 'ops': 670,\n",
       " 'exceeds': 671,\n",
       " 'expectations': 672,\n",
       " 'fun': 673,\n",
       " 'action': 674,\n",
       " 'acting': 675,\n",
       " 'pace': 676,\n",
       " 'cinematography': 677,\n",
       " 'was': 678,\n",
       " 'obviously': 679,\n",
       " 'made': 680,\n",
       " 'xxx': 681,\n",
       " 'crowd': 682,\n",
       " 'people': 683,\n",
       " 'enjoy': 684,\n",
       " 'mindless': 685,\n",
       " 'benefit': 686,\n",
       " 'decent': 687,\n",
       " 'writing': 688,\n",
       " 'heaven': 689,\n",
       " 'masterpiece': 690,\n",
       " 'feardotcom': 691,\n",
       " 'makings': 692,\n",
       " 'interesting': 693,\n",
       " 'meditation': 694,\n",
       " 'ethereal': 695,\n",
       " 'nature': 696,\n",
       " 'internet': 697,\n",
       " 'otherworldly': 698,\n",
       " 'energies': 699,\n",
       " 'could': 700,\n",
       " 'channel': 701,\n",
       " 'simply': 702,\n",
       " 'becomes': 703,\n",
       " 'routine': 704,\n",
       " 'shocker': 705,\n",
       " 'frailty': 706,\n",
       " 'written': 707,\n",
       " 'simple': 708,\n",
       " 'goddammit': 709,\n",
       " 'takes': 710,\n",
       " 'whole': 711,\n",
       " 'other': 712,\n",
       " 'freaky': 713,\n",
       " 'friday': 714,\n",
       " 'go': 715,\n",
       " 'heist': 716,\n",
       " 'hey': 717,\n",
       " 'baseball': 718,\n",
       " 'playing': 719,\n",
       " 'monkey': 720,\n",
       " 'worse': 721,\n",
       " 'friend': 722,\n",
       " 'remembers': 723,\n",
       " 'home': 724,\n",
       " 'alone': 725,\n",
       " 'sweet': 726,\n",
       " 'treasure': 727,\n",
       " 'worth': 728,\n",
       " 'equivalent': 729,\n",
       " 'lovingly': 730,\n",
       " 'rendered': 731,\n",
       " 'coffee': 732,\n",
       " 'table': 733,\n",
       " 'book': 734,\n",
       " 'house': 735,\n",
       " 'blame': 736,\n",
       " 'all': 737,\n",
       " 'men': 738,\n",
       " 'warden': 739,\n",
       " 'daughter': 740,\n",
       " 'tells': 741,\n",
       " 'her': 742,\n",
       " 'father': 743,\n",
       " 'deep': 744,\n",
       " 'sentiment': 745,\n",
       " 'broken': 746,\n",
       " 'ichi': 747,\n",
       " 'killer': 748,\n",
       " 'takashi': 749,\n",
       " 'miike': 750,\n",
       " 'japan': 751,\n",
       " 'wildest': 752,\n",
       " 'filmmaker': 753,\n",
       " 'crime': 754,\n",
       " 'fighter': 755,\n",
       " 'carrying': 756,\n",
       " 'emotional': 757,\n",
       " 'baggage': 758,\n",
       " 'batman': 759,\n",
       " 'imitation': 760,\n",
       " 'bedroom': 761,\n",
       " 'image': 762,\n",
       " 'astonishing': 763,\n",
       " 'laissez': 764,\n",
       " 'passer': 765,\n",
       " 'lilo': 766,\n",
       " 'looking': 767,\n",
       " 'leonard': 768,\n",
       " 'just': 769,\n",
       " 'kinda': 770,\n",
       " 'neutral': 771,\n",
       " 'hoping': 772,\n",
       " 'stiff': 773,\n",
       " 'wind': 774,\n",
       " 'blow': 775,\n",
       " 'uphill': 776,\n",
       " 'mib': 777,\n",
       " 'due': 778,\n",
       " 'rapid': 779,\n",
       " 'fire': 780,\n",
       " 'delivery': 781,\n",
       " 'inspired': 782,\n",
       " 'levity': 783,\n",
       " 'ca': 784,\n",
       " 'dismissed': 785,\n",
       " 'probing': 786,\n",
       " 'examination': 787,\n",
       " 'female': 788,\n",
       " 'friendship': 789,\n",
       " 'set': 790,\n",
       " 'against': 791,\n",
       " 'dynamic': 792,\n",
       " 'decades': 793,\n",
       " 'memento': 794,\n",
       " 'earmarks': 795,\n",
       " 'less': 796,\n",
       " 'vibrant': 797,\n",
       " 'jokes': 798,\n",
       " 'are': 799,\n",
       " 'lukewarm': 800,\n",
       " 'anyone': 801,\n",
       " 'really': 802,\n",
       " 'care': 803,\n",
       " 'minority': 804,\n",
       " 'report': 805,\n",
       " 'moonlight': 806,\n",
       " 'mile': 807,\n",
       " 'moretti': 808,\n",
       " 'quieter': 809,\n",
       " 'observations': 810,\n",
       " 'mostly': 811,\n",
       " 'martha': 812,\n",
       " 'bright': 813,\n",
       " 'modern': 814,\n",
       " 'day': 815,\n",
       " 'parable': 816,\n",
       " 'wears': 817,\n",
       " 'sleeve': 818,\n",
       " 'mr': 819,\n",
       " 'deeds': 820,\n",
       " 'suitable': 821,\n",
       " 'offers': 822,\n",
       " 'escapism': 823,\n",
       " 'requiring': 824,\n",
       " 'thought': 825,\n",
       " 'my': 826,\n",
       " 'god': 827,\n",
       " 'behaving': 828,\n",
       " 'idiot': 829,\n",
       " 'yes': 830,\n",
       " 'ben': 831,\n",
       " 'kingsley': 832,\n",
       " 'might': 833,\n",
       " 'notting': 834,\n",
       " 'hill': 835,\n",
       " 'o': 836,\n",
       " 'bruin': 837,\n",
       " 'where': 838,\n",
       " 'art': 839,\n",
       " 'thou': 840,\n",
       " 'cross': 841,\n",
       " 'country': 842,\n",
       " 'adventure': 843,\n",
       " 'ocean': 844,\n",
       " 'tight': 845,\n",
       " 'pants': 846,\n",
       " 'tits': 847,\n",
       " 'turn': 848,\n",
       " 'stupid': 849,\n",
       " 'um': 850,\n",
       " 'basis': 851,\n",
       " 'plot': 852,\n",
       " 'orange': 853,\n",
       " 'county': 854,\n",
       " 'refreshing': 855,\n",
       " 'change': 856,\n",
       " 'pg': 857,\n",
       " 'rating': 858,\n",
       " 'personal': 859,\n",
       " 'freedom': 860,\n",
       " 'pick': 861,\n",
       " 'pokemon': 862,\n",
       " 'pretty': 863,\n",
       " 'woman': 864,\n",
       " 'punch': 865,\n",
       " 'drunk': 866,\n",
       " 'love': 867,\n",
       " 'chocolate': 868,\n",
       " 'milk': 869,\n",
       " 'moustache': 870,\n",
       " 'queen': 871,\n",
       " 'quills': 872,\n",
       " 'read': 873,\n",
       " 'lips': 874,\n",
       " 'roger': 875,\n",
       " 'michell': 876,\n",
       " 'directs': 877,\n",
       " 'morality': 878,\n",
       " 'thriller': 879,\n",
       " 'rollerball': 880,\n",
       " 'conduct': 881,\n",
       " 'simone': 882,\n",
       " 'funky': 883,\n",
       " 'artificial': 884,\n",
       " 'creation': 885,\n",
       " 'thrives': 886,\n",
       " 'artificiality': 887,\n",
       " 'sorority': 888,\n",
       " 'boys': 889,\n",
       " 'funnier': 890,\n",
       " 'spy': 891,\n",
       " 'kids': 892,\n",
       " 'take': 893,\n",
       " 'cat': 894,\n",
       " 'talk': 895,\n",
       " 'dangerous': 896,\n",
       " 'lives': 897,\n",
       " 'altar': 898,\n",
       " 'flaws': 899,\n",
       " 'humor': 900,\n",
       " 'talented': 901,\n",
       " 'emperor': 902,\n",
       " 'clothes': 903,\n",
       " 'plan': 904,\n",
       " 'least': 905,\n",
       " 'isle': 906,\n",
       " 'sock': 907,\n",
       " 'tour': 908,\n",
       " 'de': 909,\n",
       " 'unlike': 910,\n",
       " 'kid': 911,\n",
       " 'stays': 912,\n",
       " 'picture': 913,\n",
       " 'terrifically': 914,\n",
       " 'told': 915,\n",
       " 'wrote': 916,\n",
       " 'cliff': 917,\n",
       " 'notes': 918,\n",
       " 'edition': 919,\n",
       " 'cheat': 920,\n",
       " 'mothman': 921,\n",
       " 'prophecies': 922,\n",
       " 'difficult': 923,\n",
       " 'shake': 924,\n",
       " 'conscience': 925,\n",
       " 'falls': 926,\n",
       " 'quiet': 927,\n",
       " 'saigon': 928,\n",
       " '1952': 929,\n",
       " 'sign': 930,\n",
       " 'trouble': 931,\n",
       " 'english': 932,\n",
       " 'language': 933,\n",
       " 'copy': 934,\n",
       " 'carries': 935,\n",
       " 'same': 936,\n",
       " 'strengths': 937,\n",
       " 'machine': 938,\n",
       " 'believe': 939,\n",
       " 'sense': 940,\n",
       " 'plain': 941,\n",
       " 'bored': 942,\n",
       " 'tuxedo': 943,\n",
       " 'vehicle': 944,\n",
       " 'chan': 945,\n",
       " 'mask': 946,\n",
       " 'jim': 947,\n",
       " 'carrey': 948,\n",
       " 'lion': 949,\n",
       " 'king': 950,\n",
       " 'turntable': 951,\n",
       " 'outselling': 952,\n",
       " 'electric': 953,\n",
       " 'guitar': 954,\n",
       " 'twilight': 955,\n",
       " 'zone': 956,\n",
       " 'episode': 957,\n",
       " 'two': 958,\n",
       " 'weddings': 959,\n",
       " 'funeral': 960,\n",
       " 'wait': 961,\n",
       " 'until': 962,\n",
       " 'dark': 963,\n",
       " 'waking': 964,\n",
       " 'john': 965,\n",
       " 'heroic': 966,\n",
       " 'condone': 967,\n",
       " 'recently': 968,\n",
       " 'said': 969,\n",
       " 'tortuous': 970,\n",
       " 'comment': 971,\n",
       " 'perfectly': 972,\n",
       " 'illustrates': 973,\n",
       " 'moral': 974,\n",
       " 'schizophrenia': 975,\n",
       " 'there': 976,\n",
       " 'happened': 977,\n",
       " 'philosophers': 978,\n",
       " 'filmmakers': 979,\n",
       " 'need': 980,\n",
       " 'engage': 981,\n",
       " 'oleander': 982,\n",
       " 'akin': 983,\n",
       " 'reader': 984,\n",
       " 'digest': 985,\n",
       " 'condensed': 986,\n",
       " 'source': 987,\n",
       " 'city': 988,\n",
       " 'convolution': 989,\n",
       " 'suggests': 990,\n",
       " 'snake': 991,\n",
       " 'terrible': 992,\n",
       " 'directed': 993,\n",
       " 'alan': 994,\n",
       " 'taylor': 995,\n",
       " 'napoleon': 996,\n",
       " 'journey': 997,\n",
       " 'parisian': 998,\n",
       " 'rebirth': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipline for Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the max length for each sentence in trained data\n",
    "def maxSeqLen(training_data):\n",
    "\n",
    "    total_words = 0\n",
    "    sequence_length = []\n",
    "    idx = 0\n",
    "    for index, row in training_data.iterrows():\n",
    "\n",
    "        sentence = (row['Phrase'])\n",
    "        sentence_words = sentence.split(' ')\n",
    "        len_sentence_words = len(sentence_words)\n",
    "        total_words = total_words + len_sentence_words\n",
    "\n",
    "        # get the length of the sequence of each training data\n",
    "        sequence_length.append(len_sentence_words)\n",
    "\n",
    "        if idx == 0:\n",
    "            max_seq_len = len_sentence_words\n",
    "\n",
    "\n",
    "        if len_sentence_words > max_seq_len:\n",
    "            max_seq_len = len_sentence_words\n",
    "        idx = idx + 1\n",
    "\n",
    "    avg_words = total_words/index\n",
    "    # convert to numpy array\n",
    "    sequence_length_np = np.asarray(sequence_length)\n",
    "    return max_seq_len, avg_words, sequence_length_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs from dl_sentiment that are hard coded but need to be automated\n",
    "maxSeqLength, avg_words, sequence_length = maxSeqLen(all_data)\n",
    "#numClasses = 10\n",
    "numClasses = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labels matrix for the rnn\n",
    "def tf_data_pipeline_nltk(data, word_idx, max_seq_len):\n",
    "\n",
    "    maxSeqLength = max_seq_len #Maximum length of sentence\n",
    "    no_rows = len(data)\n",
    "    ids = np.zeros((no_rows, maxSeqLength), dtype='int32')\n",
    "    # convert keys in dict to lower case\n",
    "    word_idx_lwr =  {k.lower(): v for k, v in word_idx.items()}\n",
    "    idx = 0\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "\n",
    "\n",
    "        sentence = (row['Phrase'])\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        sentence_words = tokenizer.tokenize(sentence)\n",
    "        i = 0\n",
    "        for word in sentence_words:\n",
    "            word_lwr = word.lower()\n",
    "            try:\n",
    "                ids[idx][i] =  word_idx_lwr[word_lwr]\n",
    "\n",
    "            except Exception as e:\n",
    "                if str(e) == word:\n",
    "                    ids[idx][i] = 0\n",
    "                continue\n",
    "            i = i + 1\n",
    "        idx = idx + 1\n",
    "\n",
    "    return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the label matrix for training data\n",
    "def labels_matrix(data):\n",
    "\n",
    "    labels = data['sentiment_values']\n",
    "\n",
    "    lables_float = labels.astype(float)\n",
    "\n",
    "    cats = ['0','1','2','3','4','5','6','7','8','9']\n",
    "    labels_mult = (lables_float * 10).astype(int)\n",
    "    dummies = pd.get_dummies(labels_mult, prefix='', prefix_sep='')\n",
    "    dummies = dummies.T.reindex(cats).T.fillna(0)\n",
    "    labels_matrix = dummies.iloc[:,:].values\n",
    "    return labels_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             0.5\n",
       "1         0.52778\n",
       "2         0.44444\n",
       "3         0.86111\n",
       "4         0.93056\n",
       "           ...   \n",
       "191167    0.13889\n",
       "191168    0.19444\n",
       "191169    0.51389\n",
       "191170        0.5\n",
       "191171        0.5\n",
       "Name: sentiment_values, Length: 191172, dtype: object"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['sentiment_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Training data matrix\n",
    "\n",
    "train_x = tf_data_pipeline_nltk(train_data, word_idx, maxSeqLength)\n",
    "test_x = tf_data_pipeline_nltk(test_data, word_idx, maxSeqLength)\n",
    "val_x = tf_data_pipeline_nltk(dev_data, word_idx, maxSeqLength)\n",
    "\n",
    "    \n",
    "# load labels data matrix\n",
    "train_y = labels_matrix(train_data)\n",
    "val_y = labels_matrix(dev_data)\n",
    "test_y = labels_matrix(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: \n",
      "(191172, 56)\n",
      "(191172, 10)\n",
      "Classes: \n",
      "[10]\n"
     ]
    }
   ],
   "source": [
    "# summarize size\n",
    "print(\"Training data: \")\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "\n",
    "# Summarize number of classes\n",
    "print(\"Classes: \")\n",
    "print(np.unique(train_y.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start to build a RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Bidirectional\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dropout\n",
    "import h5py\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_rnn(weight_matrix, max_words, EMBEDDING_DIM):\n",
    "    # create the model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_idx), EMBEDDING_DIM, input_length=max_words, trainable=False))\n",
    "    #model.add(Embedding(len(weight_matrix), EMBEDDING_DIM, weights=[weight_matrix], input_length=max_words, trainable=False))\n",
    "    #model.add(Embedding(len(weight_matrix), EMBEDDING_DIM, weights=[weight_matrix], input_length=max_words, trainable=False))\n",
    "    #model.add(Bidirectional(LSTM(256, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    #model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2,return_sequences=True))\n",
    "    model.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2))\n",
    "    #model.add(LSTM(512, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.50))   \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # try using different optimizers and different optimizer configs\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,train_x, train_y, test_x, test_y, val_x, val_y, batch_size, path) :\n",
    "\n",
    "    # save the best model and early stopping\n",
    "    #saveBestModel = keras.callbacks.ModelCheckpoint(path+'Train' + '/model/best_model.hdf5', monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "    #earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "\n",
    "    # Fit the model\n",
    "    #model.fit(train_x, train_y, batch_size=batch_size, epochs=25,validation_data=(val_x, val_y), callbacks=[saveBestModel, earlyStopping])\n",
    "    model.fit(train_x, train_y, batch_size=batch_size, epochs=15,validation_data=(val_x, val_y))\n",
    "    # Final evaluation of the model\n",
    "    score, acc = model.evaluate(test_x, test_y, batch_size=batch_size)\n",
    "\n",
    "    print('Test score:', score)\n",
    "    print('Test accuracy:', acc)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction of sentiment value for given sentence, top 3 average sentiment bands are taken\n",
    "# reference: https://medium.com/analytics-vidhya/sentiment-analysis-for-text-with-deep-learning-2f0a0c6472b5\n",
    "def live_test(trained_model, data, word_idx):\n",
    "\n",
    "    live_list = []\n",
    "    live_list_np = np.zeros((56,1))\n",
    "    # split the sentence into its words and remove any punctuations.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    data_sample_list = tokenizer.tokenize(data)\n",
    "\n",
    "    labels = np.array(['1','2','3','4','5','6','7','8','9','10'], dtype = \"int\")\n",
    "\n",
    "    # get index for the live stage\n",
    "    data_index = np.array([word_idx[word.lower()] if word.lower() in word_idx else 0 for word in data_sample_list])\n",
    "    data_index_np = np.array(data_index)\n",
    "    print(data_index_np)\n",
    "\n",
    "    # padded with zeros of length 56 i.e maximum length\n",
    "    padded_array = np.zeros(56) # use the def maxSeqLen(training_data) function to detemine the padding length for your data\n",
    "    padded_array[:data_index_np.shape[0]] = data_index_np\n",
    "    data_index_np_pad = padded_array.astype(int)\n",
    "    live_list.append(data_index_np_pad)\n",
    "    live_list_np = np.asarray(live_list)\n",
    "    type(live_list_np)\n",
    "\n",
    "    # get score from the model\n",
    "    score = trained_model.predict(live_list_np, batch_size=1, verbose=0)\n",
    "\n",
    "    single_score = np.round(np.argmax(score)/10, decimals=2) # maximum of the array i.e single band\n",
    "\n",
    "    # weighted score of top 3 bands\n",
    "    top_3_index = np.argsort(score)[0][-3:]\n",
    "    top_3_scores = score[0][top_3_index]\n",
    "    top_3_weights = top_3_scores/np.sum(top_3_scores)\n",
    "    single_score_dot = np.round(np.dot(top_3_index, top_3_weights)/10, decimals = 2)\n",
    "\n",
    "    #print (single_score)\n",
    "    return single_score_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 56 # max no of words in your training data\n",
    "batch_size = 2000 # batch size for training\n",
    "EMBEDDING_DIM = 300 # size of the word embeddings\n",
    "train_flag = True # set True if in training mode else False if in prediction mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 56, 300)           5417100   \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 256)               570368    \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 6,124,182\n",
      "Trainable params: 707,082\n",
      "Non-trainable params: 5,417,100\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = create_model_rnn(weight_matrix, max_words, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "96/96 [==============================] - 785s 8s/step - loss: 1.9868 - accuracy: 0.3435 - val_loss: 1.9051 - val_accuracy: 0.3633\n",
      "Epoch 2/15\n",
      "96/96 [==============================] - 1012s 11s/step - loss: 1.9213 - accuracy: 0.3586 - val_loss: 1.8539 - val_accuracy: 0.3637\n",
      "Epoch 3/15\n",
      "96/96 [==============================] - 1035s 11s/step - loss: 1.8604 - accuracy: 0.3581 - val_loss: 1.8425 - val_accuracy: 0.3637\n",
      "Epoch 4/15\n",
      "96/96 [==============================] - 1056s 11s/step - loss: 1.8459 - accuracy: 0.3589 - val_loss: 1.8285 - val_accuracy: 0.3648\n",
      "Epoch 5/15\n",
      "96/96 [==============================] - 1097s 11s/step - loss: 1.8414 - accuracy: 0.3594 - val_loss: 1.8237 - val_accuracy: 0.3656\n",
      "Epoch 6/15\n",
      "96/96 [==============================] - 1100s 11s/step - loss: 1.8362 - accuracy: 0.3583 - val_loss: 1.8209 - val_accuracy: 0.3650\n",
      "Epoch 7/15\n",
      "96/96 [==============================] - 1053s 11s/step - loss: 1.8298 - accuracy: 0.3608 - val_loss: 1.8144 - val_accuracy: 0.3654\n",
      "Epoch 8/15\n",
      "96/96 [==============================] - 1054s 11s/step - loss: 1.8310 - accuracy: 0.3600 - val_loss: 1.8090 - val_accuracy: 0.3663\n",
      "Epoch 9/15\n",
      "96/96 [==============================] - 1043s 11s/step - loss: 1.8226 - accuracy: 0.3617 - val_loss: 1.8097 - val_accuracy: 0.3654\n",
      "Epoch 10/15\n",
      "96/96 [==============================] - 1037s 11s/step - loss: 1.8253 - accuracy: 0.3606 - val_loss: 1.8081 - val_accuracy: 0.3655\n",
      "Epoch 11/15\n",
      "96/96 [==============================] - 1046s 11s/step - loss: 1.8204 - accuracy: 0.3626 - val_loss: 1.8011 - val_accuracy: 0.3695\n",
      "Epoch 12/15\n",
      "96/96 [==============================] - 1035s 11s/step - loss: 1.8143 - accuracy: 0.3637 - val_loss: 1.8178 - val_accuracy: 0.3709\n",
      "Epoch 13/15\n",
      "96/96 [==============================] - 1012s 11s/step - loss: 1.8172 - accuracy: 0.3645 - val_loss: 1.7980 - val_accuracy: 0.3690\n",
      "Epoch 14/15\n",
      "96/96 [==============================] - 1013s 11s/step - loss: 1.8039 - accuracy: 0.3673 - val_loss: 1.8062 - val_accuracy: 0.3677\n",
      "Epoch 15/15\n",
      "96/96 [==============================] - 1021s 11s/step - loss: 1.8101 - accuracy: 0.3646 - val_loss: 1.7833 - val_accuracy: 0.3726\n",
      "13/13 [==============================] - 27s 2s/step - loss: 1.7815 - accuracy: 0.3724\n",
      "Test score: 1.781495213508606\n",
      "Test accuracy: 0.3724290132522583\n"
     ]
    }
   ],
   "source": [
    "trained_model =train_model(model,train_x, train_y, test_x, test_y, val_x, val_y, batch_size, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[316  19  13   0 334]\n"
     ]
    }
   ],
   "source": [
    "# predict the sentiment value as a test of model\n",
    "data_sample = \"Great!! it is sunny today!!\"\n",
    "result = live_test(model,data_sample, word_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
